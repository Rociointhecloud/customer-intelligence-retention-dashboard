from __future__ import annotations

import sys
from datetime import datetime
from pathlib import Path

import pandas as pd
import plotly.express as px
import streamlit as st

# Ensure repo root is on sys.path so `src.*` imports work on Streamlit Cloud
REPO_ROOT = Path(__file__).resolve().parents[1]
if str(REPO_ROOT) not in sys.path:
    sys.path.insert(0, str(REPO_ROOT))

from src.config import settings
from src.modeling.inference import predict_churn_proba


ROOT = settings.root_dir
PROCESSED_DIR = ROOT / settings.data_processed_dir


# -----------------------------
# Page config
# -----------------------------
st.set_page_config(
    page_title="Customer Intelligence Dashboard",
    page_icon="ðŸ“Š",
    layout="wide",
)


# -----------------------------
# Helpers
# -----------------------------
def money(x: float) -> str:
    return f"{x:,.0f}"


def pct(x: float) -> str:
    return f"{x:.2f}%"


def last_updated(path: Path) -> str:
    ts = path.stat().st_mtime
    return datetime.fromtimestamp(ts).strftime("%Y-%m-%d %H:%M")


def risk_label(p: float) -> tuple[str, str]:
    """Returns (label, emoji) for a probability p in [0,1]."""
    if p >= 0.75:
        return "High", "ðŸ”´"
    if p >= 0.45:
        return "Medium", "ðŸŸ "
    return "Low", "ðŸŸ¢"


# -----------------------------
# Data loading
# -----------------------------
@st.cache_data
def load_data() -> tuple[pd.DataFrame, pd.DataFrame]:
    # Preferred local paths (generated by `python main.py`)
    seg_path = PROCESSED_DIR / "customer_segments.csv"
    tx_path = PROCESSED_DIR / "transactions.csv"

    # Cloud-friendly fallback (tracked in git)
    demo_seg_path = ROOT / "data" / "demo" / "customer_segments_demo.csv"
    demo_tx_path = ROOT / "data" / "demo" / "transactions_demo.csv"

    if seg_path.exists() and tx_path.exists():
        segments = pd.read_csv(seg_path)
        tx = pd.read_csv(tx_path)
        data_source = f"processed (local) Â· updated {last_updated(seg_path)}"
    elif demo_seg_path.exists() and demo_tx_path.exists():
        segments = pd.read_csv(demo_seg_path)
        tx = pd.read_csv(demo_tx_path)
        data_source = f"demo sample (cloud) Â· updated {last_updated(demo_seg_path)}"
    else:
        st.error(
            "No data found.\n\n"
            "Local: run `python main.py` to generate `data/processed/*.csv`\n"
            "Cloud: commit `data/demo/*.csv` so the app can load sample data."
        )
        st.stop()

    # Parse timestamps
    if "order_purchase_timestamp" in tx.columns:
        tx["order_purchase_timestamp"] = pd.to_datetime(
            tx["order_purchase_timestamp"], errors="coerce"
        )

    # Optional: ensure expected columns exist
    required_seg_cols = {"customer_unique_id", "segment_name", "monetary_total"}
    if not required_seg_cols.issubset(set(segments.columns)):
        st.error(
            "Segments file does not include expected columns. "
            f"Missing: {sorted(required_seg_cols - set(segments.columns))}"
        )
        st.stop()

    required_tx_cols = {"order_id", "order_purchase_timestamp"}
    if not required_tx_cols.issubset(set(tx.columns)):
        st.error(
            "Transactions file does not include expected columns. "
            f"Missing: {sorted(required_tx_cols - set(tx.columns))}"
        )
        st.stop()

    st.session_state["data_source"] = data_source
    return segments, tx


def main() -> None:
    st.title("ðŸ“Š Customer Intelligence & Retention Dashboard")
    st.caption(
        "Olist e-commerce (2016â€“2018) Â· Segmentation + Churn prediction (proxy label)"
    )

    with st.spinner("Loading data..."):
        segments, tx = load_data()

    churn_cols = [c for c in segments.columns if c.startswith("churn_")]
    churn_col = churn_cols[0] if churn_cols else None

    # Coverage
    min_date = tx["order_purchase_timestamp"].min()
    max_date = tx["order_purchase_timestamp"].max()
    days_span = int((max_date - min_date).days) if pd.notna(min_date) and pd.notna(max_date) else 0

    # Sidebar
    st.sidebar.header("Filters")
    segment_options = ["All"] + sorted(
        segments["segment_name"].dropna().unique().tolist()
    )
    selected_segment = st.sidebar.selectbox("Customer segment", segment_options)

    if pd.notna(min_date) and pd.notna(max_date):
        date_range = st.sidebar.date_input(
            "Purchase date range",
            value=(min_date.date(), max_date.date()),
            min_value=min_date.date(),
            max_value=max_date.date(),
        )
    else:
        date_range = None

    if selected_segment != "All":
        segments_filtered = segments[segments["segment_name"] == selected_segment].copy()
    else:
        segments_filtered = segments.copy()

    tx_filtered = tx.copy()
    if date_range:
        tx_filtered = tx_filtered[
            (tx_filtered["order_purchase_timestamp"].dt.date >= date_range[0])
            & (tx_filtered["order_purchase_timestamp"].dt.date <= date_range[1])
        ].copy()

    # KPIs
    total_customers = int(segments_filtered.shape[0])
    total_orders = int(tx_filtered["order_id"].nunique())
    total_revenue = float(segments_filtered["monetary_total"].sum())

    if churn_col:
        churn_rate = float(segments_filtered[churn_col].mean() * 100)
        churn_label = f"Churn (proxy {churn_col.replace('churn_', '')})"
        churn_value = pct(churn_rate)
    else:
        churn_label = "Churn (proxy)"
        churn_value = "N/A"

    k1, k2, k3, k4 = st.columns(4)
    k1.metric("Customers", f"{total_customers:,}")
    k2.metric("Delivered orders", f"{total_orders:,}")
    k3.metric("Revenue", money(total_revenue))
    k4.metric(churn_label, churn_value)

    # Data source + coverage
    data_source = st.session_state.get("data_source", "unknown")
    if pd.notna(min_date) and pd.notna(max_date):
        st.caption(
            f"Coverage: {min_date.date()} â†’ {max_date.date()} "
            f"({days_span} days) Â· Data source: {data_source}"
        )
    else:
        st.caption(f"Data source: {data_source}")

    st.divider()

    tab1, tab2, tab3, tab4 = st.tabs(["Executive", "Segments", "Predict", "Method"])

    # -----------------------------
    # Tab 1: Executive
    # -----------------------------
    with tab1:
        st.subheader("Executive summary (non-technical)")

        st.markdown(
            """
This dashboard answers 3 business questions:

1) **Who are our customers today?** (segments)  
2) **Where is revenue coming from?** (impact per segment)  
3) **Who is likely to stop buying?** (risk prediction)
            """
        )

        rev_by_seg = (
            segments_filtered.groupby("segment_name")["monetary_total"]
            .sum()
            .sort_values(ascending=False)
        )
        if len(rev_by_seg) > 0:
            rev_share = (rev_by_seg / rev_by_seg.sum() * 100).round(2)
            top_seg = rev_by_seg.index[0]
            st.info(
                f"Highest revenue segment: **{top_seg}** Â· "
                f"Revenue share: **{rev_share.iloc[0]:.2f}%**"
            )
        else:
            st.info("Not enough data for revenue share.")

        fig = px.bar(
            rev_by_seg.reset_index(),
            x="segment_name",
            y="monetary_total",
            title="Revenue by segment",
            labels={"segment_name": "Segment", "monetary_total": "Revenue"},
        )
        fig.update_layout(xaxis_tickangle=-30)
        st.plotly_chart(fig, use_container_width=True)

        st.subheader("Recommended actions")
        st.markdown(
            """
- **Inactives / Hibernating:** win-back with limited incentive + highlight fast-delivery products  
- **New Customers:** onboarding journey + second purchase trigger in 7â€“30 days  
- **Need Attention:** reduce friction (delivery / support), personalized reminders  
            """
        )

    # -----------------------------
    # Tab 2: Segments
    # -----------------------------
    with tab2:
        st.subheader("Segment distribution")

        seg_counts = segments_filtered["segment_name"].value_counts().reset_index()
        seg_counts.columns = ["segment_name", "customers"]

        fig = px.bar(
            seg_counts,
            x="segment_name",
            y="customers",
            title="Customers by segment",
            labels={"segment_name": "Segment", "customers": "Customers"},
        )
        fig.update_layout(xaxis_tickangle=-30)
        st.plotly_chart(fig, use_container_width=True)

        st.subheader("Segment table")
        if churn_col:
            summary = (
                segments_filtered.groupby("segment_name")
                .agg(
                    customers=("customer_unique_id", "count"),
                    revenue=("monetary_total", "sum"),
                    churn_rate=(churn_col, "mean"),
                )
                .sort_values("revenue", ascending=False)
            )
            summary["churn_rate"] = (summary["churn_rate"] * 100).round(2)
        else:
            summary = (
                segments_filtered.groupby("segment_name")
                .agg(
                    customers=("customer_unique_id", "count"),
                    revenue=("monetary_total", "sum"),
                )
                .sort_values("revenue", ascending=False)
            )

        st.dataframe(summary, use_container_width=True)

        st.download_button(
            "Download segment summary (CSV)",
            data=summary.reset_index().to_csv(index=False).encode("utf-8"),
            file_name="segment_summary.csv",
            mime="text/csv",
        )

    # -----------------------------
    # Tab 3: Predict
    # -----------------------------
    with tab3:
        st.subheader("Churn risk prediction (interactive)")

        st.write(
            "This is a **risk score (0â€“100%)** to prioritize outreach. "
            "It supports decisions, it is not a guarantee."
        )

        try:
            segments_scored = segments_filtered.copy()

            # 1) Predict probability (0â€“1)
            segments_scored["churn_probability"] = predict_churn_proba(segments_scored)

            # 2) Convert to percentage for UX
            segments_scored["churn_probability_pct"] = (
                segments_scored["churn_probability"] * 100
            ).clip(0, 100)

            st.divider()

            st.markdown("### Decision controls")

            cA, cB = st.columns([1, 1])
            with cA:
                threshold = st.slider(
                    "Risk threshold (%)",
                    min_value=10,
                    max_value=95,
                    value=60,
                    step=5,
                    help="Customers above this threshold will be flagged as high-risk.",
                )
            with cB:
                uplift_rate = st.slider(
                    "Expected win-back rate (%)",
                    min_value=1,
                    max_value=25,
                    value=5,
                    step=1,
                    help="Simple scenario: re-activate X% of high-risk customers.",
                )

            high_risk = segments_scored[
                segments_scored["churn_probability_pct"] >= threshold
            ].copy()

            st.markdown("### Opportunity sizing")

            total_customers_scored = int(segments_scored.shape[0])
            high_risk_customers = int(high_risk.shape[0])

            total_revenue_scored = float(segments_scored["monetary_total"].sum())
            revenue_at_risk = float(high_risk["monetary_total"].sum())
            revenue_at_risk_share = (
                (revenue_at_risk / total_revenue_scored) * 100
                if total_revenue_scored > 0
                else 0.0
            )

            projected_uplift = revenue_at_risk * (uplift_rate / 100)
            avg_risk = float(segments_scored["churn_probability_pct"].mean())

            k1, k2, k3, k4 = st.columns(4)
            k1.metric("Average churn risk", f"{avg_risk:.1f}%")
            k2.metric("High-risk customers", f"{high_risk_customers:,} / {total_customers_scored:,}")
            k3.metric("Revenue at risk", money(revenue_at_risk))
            k4.metric("Projected uplift", money(projected_uplift))

            st.caption(f"Revenue at risk share: **{revenue_at_risk_share:.1f}%** of revenue in current filters.")

            st.divider()

            st.markdown("### Distribution of churn risk (%)")

            fig = px.histogram(
                segments_scored,
                x="churn_probability_pct",
                nbins=20,
                title="Distribution of churn risk",
                labels={"churn_probability_pct": "Churn risk (%)"},
            )
            fig.update_layout(xaxis_tickformat=".0f")
            st.plotly_chart(fig, use_container_width=True)

            st.divider()

            st.markdown("### Top customers to prioritize (sample)")

            top_n = st.slider("How many customers?", 10, 300, 50, step=10)
            top = (
                segments_scored.sort_values("churn_probability_pct", ascending=False)
                .head(top_n)
                .copy()
            )

            display_cols = [
                "customer_unique_id",
                "segment_name",
                "monetary_total",
                "avg_order_value",
                "avg_review_score",
                "avg_delivery_days",
                "churn_probability_pct",
            ]

            available_cols = [c for c in display_cols if c in top.columns]
            top_display = top[available_cols].copy()
            if "churn_probability_pct" in top_display.columns:
                top_display["churn_probability_pct"] = top_display["churn_probability_pct"].round(1)
                top_display = top_display.rename(columns={"churn_probability_pct": "churn_probability_%"})

            st.dataframe(top_display, use_container_width=True)

            st.download_button(
                "Download prioritized customers (CSV)",
                data=top_display.to_csv(index=False).encode("utf-8"),
                file_name="priority_customers.csv",
                mime="text/csv",
            )

        except Exception as e:
            st.error(
                "Model artifacts not found or failed to load.\n\n"
                "Local: run `python -m src.modeling.train_churn_model`\n"
                "Cloud: ensure model artifacts are committed.\n\n"
                f"Details: {e}"
            )

    # -----------------------------
    # Tab 4: Method
    # -----------------------------
    with tab4:
        st.subheader("Methodology (portfolio transparency)")

        churn_txt = f"`{churn_col}`" if churn_col else "`churn_*` (not found in this dataset)"
        st.markdown(
            f"""
**Dataset coverage:** {min_date.date()} â†’ {max_date.date()} ({days_span} days)

**Churn label used for training (proxy):** {churn_txt}  
This is based on the last available date in the dataset (snapshot logic).

**Why we removed `recency_days` from model features:**  
Because it would *leak* the churn label definition into the model and artificially inflate performance.

**Model:** RandomForest (baseline)  
**Goal:** rank customers by risk to prioritize retention actions.
            """
        )

        st.markdown(
            """
**Limitations**
- Marketplace-heavy dataset: many customers buy once â†’ frequency is very discrete.
- Churn definition is a proxy, not a real cancellation/unsubscribe event.
- Better production evaluation would use time-based validation.
            """
        )


if __name__ == "__main__":
    main()